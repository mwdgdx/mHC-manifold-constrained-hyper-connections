# Workflow configuration for `infra_scripts/workflow.sh`.
#
# This file is `source`d by bash. Keep it to simple KEY=VALUE pairs.
# No secrets (no API keys). Prefer W&B credentials via `~/.netrc` on the pod.

############################################
# Constitutional Controls
############################################

# Constitution version marker consumed by workflow guards.
WF_CONSTITUTION_VERSION="1"

# Canonical config enforcement:
# - 0 (default): only infra_scripts/workflow.cfg is allowed for local commands.
# - 1: permit WORKFLOW_CONFIG overrides.
WF_ALLOW_OVERRIDE="0"

# Noninteractive safety guard:
# - 1 (default): in non-TTY mode, commands that may prompt must fail fast unless
#   explicit noninteractive consent is configured.
WF_REQUIRE_NONINTERACTIVE_SAFE="1"

# Default teardown policy for future composite flow orchestration.
# Valid values: keep, delete.
WF_DEFAULT_TEARDOWN="delete"

# Local evidence root used by `flow` phase artifacts.
WF_FLOW_EVIDENCE_DIR="artifacts/pod_logs/_flows"

# Checklist controls for `bash infra_scripts/workflow.sh flow`.
# This file is updated live during phase execution and reset at flow end by default.
WF_CHECKLIST_PATH="infra_scripts/workflow.checklist.md"
WF_CHECKLIST_RESET_ON_END="1"

############################################
# Platform / Remote Host
############################################

# Preferred: Lium pod identifier (pod name/ID or index from `lium ps`).
# If set, the workflow uses `lium exec/scp` instead of `ssh/scp`.
LIUM_TARGET="my-gpu-pod"

# Optional fallback: SSH host alias (e.g. from ~/.ssh/config).
OPS_DEFAULT_HOST="lium"

############################################
# Pod Provisioning (optional, Lium)
############################################

# If you want `pod-up` to work, fill these out.
LIUM_POD_NAME="my-gpu-pod"

# If set, this is passed as positional EXECUTOR_ID to `lium up`.
LIUM_EXECUTOR_ID=""

# Otherwise, the workflow uses filters.
LIUM_GPU="NVIDIA A100-SXM4-80GB"
LIUM_COUNT="8"
LIUM_COUNTRY=""
LIUM_PORTS=""

LIUM_TTL="2h"

# Volume attach spec for `lium up`.
# Examples:
# - LIUM_VOLUME="id:<HUID>"
# - LIUM_VOLUME="new:name=my_volume"
LIUM_VOLUME="id:calm-orbit-4b"

# If 1, passes `--yes` to `lium up`.
LIUM_YES="1"

# Path on the pod where the workflow config should be installed.
# This is intended to live on a persistent volume.
REMOTE_ENV_PATH="/mnt/project.env"

############################################
# Remote Layout (pod)
############################################

# Where the repo should live on fast local disk.
OPS_REMOTE_REPO="/root/work/mHC-manifold-constrained-hyper-connections"

# Durable output root (each run writes to ${OPS_REMOTE_OUTPUTS_DIR}/<run_id>/).
OPS_REMOTE_OUTPUTS_DIR="/mnt/pod_artifacts/outputs"

# Durable dataset root (FineWeb10B shards).
DATA_DIR="/mnt/data/fineweb10B"

# Optional durable cache roots.
HF_HOME="/mnt/hf"

############################################
# Workflow Session / Tasks
############################################

# Default tmux session used for tracked tasks (non-sweep long-running actions).
WF_TMUX_SESSION="wf"

# Default timeout for tracked tasks started via `task-run`.
# 0 disables timeouts.
TASK_TIMEOUT_SECS="0"

# Default timeout for each sweep run started by `sweep-start`.
# 0 disables timeouts.
RUN_TIMEOUT_SECS="0"

############################################
# Run Artifact Sync (optional)
############################################

# Where training should write artifacts:
# - durable: write directly to ${OPS_REMOTE_OUTPUTS_DIR}/<run_id>/ (safest)
# - local_sync: write to local disk and continuously rsync into the durable run dir
RUN_OUT_MODE="durable"

# Only used when RUN_OUT_MODE=local_sync.
# Default points at the fast local repo disk.
RUN_OUT_LOCAL_ROOT="${OPS_REMOTE_REPO}/examples/nanogpt/out"

# Only used when RUN_OUT_MODE=local_sync.
# How often to rsync local artifacts into the durable run dir.
SYNC_INTERVAL_SECS="60"

############################################
# Python Environment (remote)
############################################

# Python executable to use on the pod for venv creation and installs.
# If your image provides torch in a non-default python (e.g. conda), point this
# at that python.
REMOTE_PYTHON_BIN="python3"

# Workflow FSM controls:
# - If WF_FSM_ENFORCE=1, commands enforce legal action/state transitions.
# - Remote state persists at WF_STATE_FILE, or defaults to
#   ${OPS_REMOTE_OUTPUTS_DIR}/_control/workflow_state.json when empty.
WF_FSM_ENFORCE="1"
WF_STATE_FILE=""

# If 1, delete and recreate the venv during `checkout`.
VENV_RECREATE="0"

############################################
# Repo Checkout
############################################

# REQUIRED: Git URL for the repo to clone on the pod.
# Examples:
# - REPO_URL="git@github.com:<org>/<repo>.git"
# - REPO_URL="https://github.com/<org>/<repo>.git"
REPO_URL="https://github.com/tokenbender/mHC-manifold-constrained-hyper-connections.git"

# Choose ONE of:
# - CHECKOUT_BRANCH (recommended for normal runs)
# - CHECKOUT_PR (GitHub PR number; uses `pull/<n>/head` fetch)
CHECKOUT_BRANCH="main"
CHECKOUT_PR=""

# Optional: pin the exact git SHA expected after checkout.
# Useful to guarantee "remote == local" after you push your branch.
EXPECT_GIT_SHA=""

# If 1, `checkout` will hard-reset tracked files in the remote repo before pulling.
# Leave this off by default; prefer fresh pods and a clean checkout.
CHECKOUT_FORCE_CLEAN="0"

############################################
# Bootstrap Helpers (optional)
############################################

# Optional pod bootstrap script that may exist on the persistent volume.
BOOTSTRAP_SCRIPT="/mnt/bootstrap-pod.sh"

# Optional script that writes ~/.netrc for W&B (preferred over exporting WANDB_API_KEY).
WANDB_SETUP_SCRIPT="/mnt/set-wandb-api-key.sh"

# If 1, run WANDB_SETUP_SCRIPT even when SWEEP_WANDB=0.
RUN_WANDB_SETUP="0"

# If 1, the workflow will attempt to install missing prereqs via apt-get.
AUTO_INSTALL_PREREQS="1"
APT_PACKAGES="git tmux rsync curl python3 python3-venv"

# If 1, auto-download minimal FineWeb10B shards when missing.
# Note: if DATA_DIR is under /mnt and /mnt is mounted via s3fs, the workflow will
# stage the download on local disk and then copy completed shards into DATA_DIR.
DOWNLOAD_FINEWEB="0"

############################################
# Sweep Configuration
############################################

# Sweep manifest path (local).
# `sweep-start` uploads this file to `${OPS_REMOTE_OUTPUTS_DIR}/_manifests/sweep-latest.csv`
# and runs the sweep from that remote manifest path (to keep the remote git checkout clean).
# You can override this per run without editing config:
#   bash infra_scripts/workflow.sh sweep-start --csv sweeps/bat.csv
# You can generate a starter CSV with: `bash infra_scripts/workflow.sh sweep-csv-template`.
SWEEP_CSV="sweeps/bat.csv"

# Optional filters.
SWEEP_MATCH=""
SWEEP_START_AT=""
SWEEP_LIMIT=""

# If 1, rerun even if summary.json exists.
SWEEP_FORCE="0"

# Sweep tmux session (single window runs sequential DDP runs).
SWEEP_TMUX_SESSION="sweeps"

# Sweep launcher:
# - Each CSV row is launched with torchrun using *all visible GPUs*.
# - If you want to restrict GPU visibility, set CUDA_VISIBLE_DEVICES (e.g. "0,1,2,3")
#   in your pod environment before starting the sweep.

# W&B controls.
SWEEP_WANDB="0"         # 1=enable, 0=disable
WANDB_PROJECT=""        # optional override
WANDB_GROUP=""          # optional override (if empty, workflow auto-generates)

############################################
# Local Fetch
############################################

LOCAL_ARTIFACTS_DIR="artifacts/pod_logs"

# Artifact fetch policy:
#   0 => exclude checkpoint payloads (`ckpt.pt`, `*.ckpt`, `checkpoints/`) from `fetch-run`
#   1 => include full run payloads
FETCH_INCLUDE_CHECKPOINTS="0"
